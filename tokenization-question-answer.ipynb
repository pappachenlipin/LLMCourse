{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"context = \"\"\"\nðŸ¤— Transformers is backed by the three most popular deep learning libraries â€” Jax, PyTorch, and TensorFlow â€” with a seamless integration\nbetween them. It's straightforward to train your models with one before loading them for inference with the other.\n\"\"\"\nquestion = \"Which deep learning libraries back ðŸ¤— Transformers?\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T00:02:41.802770Z","iopub.execute_input":"2025-09-18T00:02:41.802958Z","iopub.status.idle":"2025-09-18T00:02:41.809837Z","shell.execute_reply.started":"2025-09-18T00:02:41.802941Z","shell.execute_reply":"2025-09-18T00:02:41.809064Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from transformers import AutoTokenizer\nmodel_checkpoint = \"distilbert-base-cased-distilled-squad\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T00:04:58.665408Z","iopub.execute_input":"2025-09-18T00:04:58.665686Z","iopub.status.idle":"2025-09-18T00:05:13.659035Z","shell.execute_reply.started":"2025-09-18T00:04:58.665668Z","shell.execute_reply":"2025-09-18T00:05:13.658458Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"590f1d19df48486ca0c9f82b9107e67a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0528546a71a469faf2865eb75700d77"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62ff33c7a652492b8c38e8c3e4c35de3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73861b2544a74d7e8bc8489100693a71"}},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"inputs = tokenizer(question, context, return_tensors=\"pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T00:10:15.927743Z","iopub.execute_input":"2025-09-18T00:10:15.928020Z","iopub.status.idle":"2025-09-18T00:10:15.986001Z","shell.execute_reply.started":"2025-09-18T00:10:15.927999Z","shell.execute_reply":"2025-09-18T00:10:15.985296Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"inputs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T00:10:21.327654Z","iopub.execute_input":"2025-09-18T00:10:21.327895Z","iopub.status.idle":"2025-09-18T00:10:21.374027Z","shell.execute_reply.started":"2025-09-18T00:10:21.327877Z","shell.execute_reply":"2025-09-18T00:10:21.373477Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([[  101,  5979,  1996,  3776,  9818,  1171,   100, 25267,   136,   102,\n           100, 25267,  1110,  5534,  1118,  1103,  1210,  1211,  1927,  1996,\n          3776,  9818,   783, 13612,   117,   153,  1183,  1942,  1766,  1732,\n           117,  1105,  5157, 21484,  2271,  6737,   783,  1114,   170,  2343,\n          1306,  2008,  9111,  1206,  1172,   119,  1135,   112,   188, 21546,\n          1106,  2669,  1240,  3584,  1114,  1141,  1196, 10745,  1172,  1111,\n          1107, 16792,  1114,  1103,  1168,   119,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"len(inputs.sequence_ids())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T00:12:56.755014Z","iopub.execute_input":"2025-09-18T00:12:56.755811Z","iopub.status.idle":"2025-09-18T00:12:56.760395Z","shell.execute_reply.started":"2025-09-18T00:12:56.755787Z","shell.execute_reply":"2025-09-18T00:12:56.759748Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"67"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"inputs.input_ids.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T00:13:25.223607Z","iopub.execute_input":"2025-09-18T00:13:25.223885Z","iopub.status.idle":"2025-09-18T00:13:25.229045Z","shell.execute_reply.started":"2025-09-18T00:13:25.223864Z","shell.execute_reply":"2025-09-18T00:13:25.228460Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 67])"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"from transformers import AutoModelForQuestionAnswering\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T00:27:08.452479Z","iopub.execute_input":"2025-09-18T00:27:08.452988Z","iopub.status.idle":"2025-09-18T00:27:38.778082Z","shell.execute_reply.started":"2025-09-18T00:27:08.452966Z","shell.execute_reply":"2025-09-18T00:27:38.777285Z"}},"outputs":[{"name":"stderr","text":"2025-09-18 00:27:18.734542: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1758155239.125283      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1758155239.228689      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c420916601a4c6e9e41d3ad381e6835"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"outputs = model(**inputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T00:28:11.607011Z","iopub.execute_input":"2025-09-18T00:28:11.607739Z","iopub.status.idle":"2025-09-18T00:28:11.691803Z","shell.execute_reply.started":"2025-09-18T00:28:11.607713Z","shell.execute_reply":"2025-09-18T00:28:11.691182Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"start_logits, end_logits = outputs.start_logits, outputs.end_logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T00:31:18.824993Z","iopub.execute_input":"2025-09-18T00:31:18.825381Z","iopub.status.idle":"2025-09-18T00:31:18.829786Z","shell.execute_reply.started":"2025-09-18T00:31:18.825359Z","shell.execute_reply":"2025-09-18T00:31:18.828967Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"start_logits.shape, end_logits.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T00:31:32.729304Z","iopub.execute_input":"2025-09-18T00:31:32.730108Z","iopub.status.idle":"2025-09-18T00:31:32.735060Z","shell.execute_reply.started":"2025-09-18T00:31:32.730070Z","shell.execute_reply":"2025-09-18T00:31:32.734201Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"(torch.Size([1, 67]), torch.Size([1, 67]))"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"import torch\nsequence_ids = inputs.sequence_ids()\n# Mask everything apart from the tokens of the context\nmask = [i != 1 for i in sequence_ids]\n# Unmask the [CLS] token\nmask[0] = False\nmask = torch.tensor(mask)[None]\n\nstart_logits[mask] = -10000\nend_logits[mask] = -10000","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T00:31:45.624544Z","iopub.execute_input":"2025-09-18T00:31:45.624808Z","iopub.status.idle":"2025-09-18T00:31:45.629703Z","shell.execute_reply.started":"2025-09-18T00:31:45.624789Z","shell.execute_reply":"2025-09-18T00:31:45.629105Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"start_prob = torch.nn.functional.softmax(start_logits, dim=-1)[-1]\nend_prob = torch.nn.functional.softmax(end_logits, dim=-1)[-1]\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T00:34:33.930947Z","iopub.execute_input":"2025-09-18T00:34:33.931267Z","iopub.status.idle":"2025-09-18T00:34:33.936649Z","shell.execute_reply.started":"2025-09-18T00:34:33.931244Z","shell.execute_reply":"2025-09-18T00:34:33.935891Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"scores = start_prob[:, None] * end_prob[None, :]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T00:35:02.280697Z","iopub.execute_input":"2025-09-18T00:35:02.280950Z","iopub.status.idle":"2025-09-18T00:35:02.285482Z","shell.execute_reply.started":"2025-09-18T00:35:02.280934Z","shell.execute_reply":"2025-09-18T00:35:02.284390Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"scores = torch.triu(scores)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T00:37:05.028590Z","iopub.execute_input":"2025-09-18T00:37:05.029432Z","iopub.status.idle":"2025-09-18T00:37:05.033031Z","shell.execute_reply.started":"2025-09-18T00:37:05.029395Z","shell.execute_reply":"2025-09-18T00:37:05.032194Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"scores.argmax()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T00:41:04.231326Z","iopub.execute_input":"2025-09-18T00:41:04.231629Z","iopub.status.idle":"2025-09-18T00:41:04.236979Z","shell.execute_reply.started":"2025-09-18T00:41:04.231609Z","shell.execute_reply":"2025-09-18T00:41:04.236454Z"}},"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"tensor(1576)"},"metadata":{}}],"execution_count":43},{"cell_type":"code","source":"torch.flatten(scores)[1576]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T00:42:11.282114Z","iopub.execute_input":"2025-09-18T00:42:11.282865Z","iopub.status.idle":"2025-09-18T00:42:11.288019Z","shell.execute_reply.started":"2025-09-18T00:42:11.282839Z","shell.execute_reply":"2025-09-18T00:42:11.287473Z"}},"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"tensor(0.9803, grad_fn=<SelectBackward0>)"},"metadata":{}}],"execution_count":45},{"cell_type":"code","source":"max_index = scores.argmax().item()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T00:37:07.798743Z","iopub.execute_input":"2025-09-18T00:37:07.799490Z","iopub.status.idle":"2025-09-18T00:37:07.802929Z","shell.execute_reply.started":"2025-09-18T00:37:07.799466Z","shell.execute_reply":"2025-09-18T00:37:07.802350Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"max_index","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T00:37:12.130823Z","iopub.execute_input":"2025-09-18T00:37:12.131428Z","iopub.status.idle":"2025-09-18T00:37:12.135760Z","shell.execute_reply.started":"2025-09-18T00:37:12.131404Z","shell.execute_reply":"2025-09-18T00:37:12.134998Z"}},"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"1576"},"metadata":{}}],"execution_count":37},{"cell_type":"code","source":"start_index = max_index // scores.shape[1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T00:44:57.149307Z","iopub.execute_input":"2025-09-18T00:44:57.149599Z","iopub.status.idle":"2025-09-18T00:44:57.153546Z","shell.execute_reply.started":"2025-09-18T00:44:57.149578Z","shell.execute_reply":"2025-09-18T00:44:57.152746Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"end_index= max_index % scores.shape[1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T00:45:04.838186Z","iopub.execute_input":"2025-09-18T00:45:04.838783Z","iopub.status.idle":"2025-09-18T00:45:04.842256Z","shell.execute_reply.started":"2025-09-18T00:45:04.838761Z","shell.execute_reply":"2025-09-18T00:45:04.841529Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"inputs_with_offsets = tokenizer(question, context, return_offsets_mapping=True)\noffsets = inputs_with_offsets[\"offset_mapping\"]\n\nstart_char, _ = offsets[start_index]\n_, end_char = offsets[end_index]\nanswer = context[start_char:end_char]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T00:45:06.569219Z","iopub.execute_input":"2025-09-18T00:45:06.569509Z","iopub.status.idle":"2025-09-18T00:45:06.573882Z","shell.execute_reply.started":"2025-09-18T00:45:06.569487Z","shell.execute_reply":"2025-09-18T00:45:06.573318Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"answer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T00:45:11.332877Z","iopub.execute_input":"2025-09-18T00:45:11.333162Z","iopub.status.idle":"2025-09-18T00:45:11.337938Z","shell.execute_reply.started":"2025-09-18T00:45:11.333142Z","shell.execute_reply":"2025-09-18T00:45:11.337180Z"}},"outputs":[{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"'Jax, PyTorch, and TensorFlow'"},"metadata":{}}],"execution_count":50},{"cell_type":"code","source":"sentence = \"This sentence is not too long but we are going to split it anyway.\"\ninputs = tokenizer(\n    sentence, truncation=True, return_overflowing_tokens=True, max_length=6, stride=2\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T00:49:12.573400Z","iopub.execute_input":"2025-09-18T00:49:12.573694Z","iopub.status.idle":"2025-09-18T00:49:12.578477Z","shell.execute_reply.started":"2025-09-18T00:49:12.573676Z","shell.execute_reply":"2025-09-18T00:49:12.577744Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T01:09:26.310954Z","iopub.execute_input":"2025-09-18T01:09:26.311242Z","iopub.status.idle":"2025-09-18T01:09:26.315996Z","shell.execute_reply.started":"2025-09-18T01:09:26.311220Z","shell.execute_reply":"2025-09-18T01:09:26.315312Z"}},"outputs":[{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"{'input_ids': [[101, 1188, 5650, 1110, 1136, 102], [101, 1110, 1136, 1315, 1263, 102], [101, 1315, 1263, 1133, 1195, 102], [101, 1133, 1195, 1132, 1280, 102], [101, 1132, 1280, 1106, 3325, 102], [101, 1106, 3325, 1122, 4050, 102], [101, 1122, 4050, 119, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1]], 'overflow_to_sample_mapping': [0, 0, 0, 0, 0, 0, 0]}"},"metadata":{}}],"execution_count":59},{"cell_type":"code","source":"1+1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T01:09:15.003467Z","iopub.execute_input":"2025-09-18T01:09:15.004006Z","iopub.status.idle":"2025-09-18T01:09:15.008772Z","shell.execute_reply.started":"2025-09-18T01:09:15.003981Z","shell.execute_reply":"2025-09-18T01:09:15.008179Z"}},"outputs":[{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"2"},"metadata":{}}],"execution_count":58},{"cell_type":"code","source":"for i in inputs[\"input_ids\"]:\n    print(tokenizer.decode(i))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T01:10:07.243951Z","iopub.execute_input":"2025-09-18T01:10:07.244515Z","iopub.status.idle":"2025-09-18T01:10:07.249120Z","shell.execute_reply.started":"2025-09-18T01:10:07.244489Z","shell.execute_reply":"2025-09-18T01:10:07.248454Z"}},"outputs":[{"name":"stdout","text":"[CLS] This sentence is not [SEP]\n[CLS] is not too long [SEP]\n[CLS] too long but we [SEP]\n[CLS] but we are going [SEP]\n[CLS] are going to split [SEP]\n[CLS] to split it anyway [SEP]\n[CLS] it anyway. [SEP]\n","output_type":"stream"}],"execution_count":60},{"cell_type":"code","source":"long_context = \"\"\"\nðŸ¤— Transformers: State of the Art NLP\n\nðŸ¤— Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,\nquestion answering, summarization, translation, text generation and more in over 100 languages.\nIts aim is to make cutting-edge NLP easier to use for everyone.\n\nðŸ¤— Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and\nthen share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and\ncan be modified to enable quick research experiments.\n\nWhy should I use transformers?\n\n1. Easy-to-use state-of-the-art models:\n  - High performance on NLU and NLG tasks.\n  - Low barrier to entry for educators and practitioners.\n  - Few user-facing abstractions with just three classes to learn.\n  - A unified API for using all our pretrained models.\n  - Lower compute costs, smaller carbon footprint:\n\n2. Researchers can share trained models instead of always retraining.\n  - Practitioners can reduce compute time and production costs.\n  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.\n\n3. Choose the right framework for every part of a model's lifetime:\n  - Train state-of-the-art models in 3 lines of code.\n  - Move a single model between TF2.0/PyTorch frameworks at will.\n  - Seamlessly pick the right framework for training, evaluation and production.\n\n4. Easily customize a model or an example to your needs:\n  - We provide examples for each architecture to reproduce the results published by its original authors.\n  - Model internals are exposed as consistently as possible.\n  - Model files can be used independently of the library for quick experiments.\n\nðŸ¤— Transformers is backed by the three most popular deep learning libraries â€” Jax, PyTorch and TensorFlow â€” with a seamless integration\nbetween them. It's straightforward to train your models with one before loading them for inference with the other.\n\"\"\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T01:16:32.175009Z","iopub.execute_input":"2025-09-18T01:16:32.175759Z","iopub.status.idle":"2025-09-18T01:16:32.179787Z","shell.execute_reply.started":"2025-09-18T01:16:32.175735Z","shell.execute_reply":"2025-09-18T01:16:32.179171Z"}},"outputs":[],"execution_count":61},{"cell_type":"code","source":"inputs = tokenizer(question, long_context, max_length=384, truncation=\"only_second\")\nprint(tokenizer.decode(inputs[\"input_ids\"]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T01:19:34.987098Z","iopub.execute_input":"2025-09-18T01:19:34.987724Z","iopub.status.idle":"2025-09-18T01:19:34.994894Z","shell.execute_reply.started":"2025-09-18T01:19:34.987698Z","shell.execute_reply":"2025-09-18T01:19:34.994172Z"}},"outputs":[{"name":"stdout","text":"[CLS] Which deep learning libraries back [UNK] Transformers? [SEP] [UNK] Transformers : State of the Art NLP [UNK] Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction, question answering, summarization, translation, text generation and more in over 100 languages. Its aim is to make cutting - edge NLP easier to use for everyone. [UNK] Transformers provides APIs to quickly download and use those pretrained models on a given text, fine - tune them on your own datasets and then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and can be modified to enable quick research experiments. Why should I use transformers? 1. Easy - to - use state - of - the - art models : - High performance on NLU and NLG tasks. - Low barrier to entry for educators and practitioners. - Few user - facing abstractions with just three classes to learn. - A unified API for using all our pretrained models. - Lower compute costs, smaller carbon footprint : 2. Researchers can share trained models instead of always retraining. - Practitioners can reduce compute time and production costs. - Dozens of architectures with over 10, 000 pretrained models, some in more than 100 languages. 3. Choose the right framework for every part of a model ' s lifetime : - Train state - of - the - art models in 3 lines of code. - Move a single model between TF2. 0 / PyTorch frameworks at will. - Seamlessly pick the right framework for training, evaluation and production. 4. Easily customize a model or an example to your needs : - We provide examples for each architecture to reproduce the results published by its original authors. - Model internal [SEP]\n","output_type":"stream"}],"execution_count":62},{"cell_type":"code","source":"inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T01:19:49.345790Z","iopub.execute_input":"2025-09-18T01:19:49.346055Z","iopub.status.idle":"2025-09-18T01:19:49.350816Z","shell.execute_reply.started":"2025-09-18T01:19:49.346038Z","shell.execute_reply":"2025-09-18T01:19:49.350275Z"}},"outputs":[{"execution_count":63,"output_type":"execute_result","data":{"text/plain":"{'input_ids': [101, 5979, 1996, 3776, 9818, 1171, 100, 25267, 136, 102, 100, 25267, 131, 1426, 1104, 1103, 2051, 21239, 2101, 100, 25267, 2790, 4674, 1104, 3073, 4487, 9044, 3584, 1106, 3870, 8249, 1113, 6685, 1216, 1112, 5393, 117, 1869, 16026, 117, 2304, 10937, 117, 7584, 7317, 2734, 117, 5179, 117, 3087, 3964, 1105, 1167, 1107, 1166, 1620, 3483, 119, 2098, 6457, 1110, 1106, 1294, 5910, 118, 2652, 21239, 2101, 5477, 1106, 1329, 1111, 2490, 119, 100, 25267, 2790, 20480, 1116, 1106, 1976, 9133, 1105, 1329, 1343, 3073, 4487, 9044, 3584, 1113, 170, 1549, 3087, 117, 2503, 118, 9253, 1172, 1113, 1240, 1319, 2233, 27948, 1105, 1173, 2934, 1172, 1114, 1103, 1661, 1113, 1412, 2235, 10960, 119, 1335, 1103, 1269, 1159, 117, 1296, 185, 25669, 8613, 13196, 13682, 1126, 4220, 1110, 3106, 2484, 20717, 1673, 1105, 1169, 1129, 5847, 1106, 9396, 3613, 1844, 7857, 119, 2009, 1431, 146, 1329, 11303, 1468, 136, 122, 119, 12167, 118, 1106, 118, 1329, 1352, 118, 1104, 118, 1103, 118, 1893, 3584, 131, 118, 1693, 2099, 1113, 21239, 2591, 1105, 21239, 2349, 8249, 119, 118, 8274, 9391, 1106, 3990, 1111, 24937, 1105, 16681, 119, 118, 17751, 4795, 118, 4749, 11108, 5266, 1114, 1198, 1210, 3553, 1106, 3858, 119, 118, 138, 13943, 20480, 1111, 1606, 1155, 1412, 3073, 4487, 9044, 3584, 119, 118, 5738, 3254, 22662, 4692, 117, 2964, 6302, 2555, 10988, 131, 123, 119, 26982, 1169, 2934, 3972, 3584, 1939, 1104, 1579, 1231, 4487, 16534, 119, 118, 153, 19366, 3121, 2116, 1468, 1169, 4851, 3254, 22662, 1159, 1105, 1707, 4692, 119, 118, 2091, 10947, 1116, 1104, 4220, 1116, 1114, 1166, 1275, 117, 1288, 3073, 4487, 9044, 3584, 117, 1199, 1107, 1167, 1190, 1620, 3483, 119, 124, 119, 22964, 6787, 1103, 1268, 8297, 1111, 1451, 1226, 1104, 170, 2235, 112, 188, 7218, 131, 118, 9791, 1352, 118, 1104, 118, 1103, 118, 1893, 3584, 1107, 124, 2442, 1104, 3463, 119, 118, 15729, 170, 1423, 2235, 1206, 157, 2271, 1477, 119, 121, 120, 153, 1183, 1942, 1766, 1732, 8297, 1116, 1120, 1209, 119, 118, 3017, 1306, 8709, 3368, 1103, 1268, 8297, 1111, 2013, 117, 10540, 1105, 1707, 119, 125, 119, 142, 20158, 8156, 3708, 170, 2235, 1137, 1126, 1859, 1106, 1240, 2993, 131, 118, 1284, 2194, 5136, 1111, 1296, 4220, 1106, 23577, 1103, 2686, 1502, 1118, 1157, 1560, 5752, 119, 118, 6747, 4422, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"},"metadata":{}}],"execution_count":63},{"cell_type":"code","source":"sentences = [\n    \"This sentence is not too long but we are going to split it anyway.\",\n    \"This sentence is shorter but will still get split.\",\n]\ninputs = tokenizer(\n    sentences, truncation=True, return_overflowing_tokens=True, max_length=6, stride=2\n)\n\nprint(inputs[\"overflow_to_sample_mapping\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T01:20:23.886072Z","iopub.execute_input":"2025-09-18T01:20:23.886370Z","iopub.status.idle":"2025-09-18T01:20:23.891845Z","shell.execute_reply.started":"2025-09-18T01:20:23.886351Z","shell.execute_reply":"2025-09-18T01:20:23.891057Z"}},"outputs":[{"name":"stdout","text":"[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]\n","output_type":"stream"}],"execution_count":64},{"cell_type":"code","source":"inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T01:20:42.266364Z","iopub.execute_input":"2025-09-18T01:20:42.266604Z","iopub.status.idle":"2025-09-18T01:20:42.271250Z","shell.execute_reply.started":"2025-09-18T01:20:42.266587Z","shell.execute_reply":"2025-09-18T01:20:42.270497Z"}},"outputs":[{"execution_count":65,"output_type":"execute_result","data":{"text/plain":"{'input_ids': [[101, 1188, 5650, 1110, 1136, 102], [101, 1110, 1136, 1315, 1263, 102], [101, 1315, 1263, 1133, 1195, 102], [101, 1133, 1195, 1132, 1280, 102], [101, 1132, 1280, 1106, 3325, 102], [101, 1106, 3325, 1122, 4050, 102], [101, 1122, 4050, 119, 102], [101, 1188, 5650, 1110, 7681, 102], [101, 1110, 7681, 1133, 1209, 102], [101, 1133, 1209, 1253, 1243, 102], [101, 1253, 1243, 3325, 119, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]], 'overflow_to_sample_mapping': [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]}"},"metadata":{}}],"execution_count":65},{"cell_type":"code","source":"inputs = tokenizer(\n    question,\n    long_context,\n    stride=128,\n    max_length=384,\n    padding=\"longest\",\n    truncation=\"only_second\",\n    return_overflowing_tokens=True,\n    return_offsets_mapping=True,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T01:23:31.407532Z","iopub.execute_input":"2025-09-18T01:23:31.408138Z","iopub.status.idle":"2025-09-18T01:23:31.413636Z","shell.execute_reply.started":"2025-09-18T01:23:31.408100Z","shell.execute_reply":"2025-09-18T01:23:31.413068Z"}},"outputs":[],"execution_count":66},{"cell_type":"code","source":"_ = inputs.pop(\"overflow_to_sample_mapping\")\noffsets = inputs.pop(\"offset_mapping\")\n\ninputs = inputs.convert_to_tensors(\"pt\")\nprint(inputs[\"input_ids\"].shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T01:24:20.214500Z","iopub.execute_input":"2025-09-18T01:24:20.215092Z","iopub.status.idle":"2025-09-18T01:24:20.220103Z","shell.execute_reply.started":"2025-09-18T01:24:20.215061Z","shell.execute_reply":"2025-09-18T01:24:20.219423Z"}},"outputs":[{"name":"stdout","text":"torch.Size([2, 384])\n","output_type":"stream"}],"execution_count":67},{"cell_type":"code","source":"for id in inputs['input_ids']:\n    print(tokenizer.decode(id))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T01:26:00.762531Z","iopub.execute_input":"2025-09-18T01:26:00.763422Z","iopub.status.idle":"2025-09-18T01:26:00.768811Z","shell.execute_reply.started":"2025-09-18T01:26:00.763394Z","shell.execute_reply":"2025-09-18T01:26:00.768038Z"}},"outputs":[{"name":"stdout","text":"[CLS] Which deep learning libraries back [UNK] Transformers? [SEP] [UNK] Transformers : State of the Art NLP [UNK] Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction, question answering, summarization, translation, text generation and more in over 100 languages. Its aim is to make cutting - edge NLP easier to use for everyone. [UNK] Transformers provides APIs to quickly download and use those pretrained models on a given text, fine - tune them on your own datasets and then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and can be modified to enable quick research experiments. Why should I use transformers? 1. Easy - to - use state - of - the - art models : - High performance on NLU and NLG tasks. - Low barrier to entry for educators and practitioners. - Few user - facing abstractions with just three classes to learn. - A unified API for using all our pretrained models. - Lower compute costs, smaller carbon footprint : 2. Researchers can share trained models instead of always retraining. - Practitioners can reduce compute time and production costs. - Dozens of architectures with over 10, 000 pretrained models, some in more than 100 languages. 3. Choose the right framework for every part of a model ' s lifetime : - Train state - of - the - art models in 3 lines of code. - Move a single model between TF2. 0 / PyTorch frameworks at will. - Seamlessly pick the right framework for training, evaluation and production. 4. Easily customize a model or an example to your needs : - We provide examples for each architecture to reproduce the results published by its original authors. - Model internal [SEP]\n[CLS] Which deep learning libraries back [UNK] Transformers? [SEP] Dozens of architectures with over 10, 000 pretrained models, some in more than 100 languages. 3. Choose the right framework for every part of a model ' s lifetime : - Train state - of - the - art models in 3 lines of code. - Move a single model between TF2. 0 / PyTorch frameworks at will. - Seamlessly pick the right framework for training, evaluation and production. 4. Easily customize a model or an example to your needs : - We provide examples for each architecture to reproduce the results published by its original authors. - Model internals are exposed as consistently as possible. - Model files can be used independently of the library for quick experiments. [UNK] Transformers is backed by the three most popular deep learning libraries â€” Jax, PyTorch and TensorFlow â€” with a seamless integration between them. It ' s straightforward to train your models with one before loading them for inference with the other. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","output_type":"stream"}],"execution_count":69},{"cell_type":"code","source":"outputs = model(**inputs)\n\nstart_logits = outputs.start_logits\nend_logits = outputs.end_logits\nprint(start_logits.shape, end_logits.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T01:26:47.603342Z","iopub.execute_input":"2025-09-18T01:26:47.603591Z","iopub.status.idle":"2025-09-18T01:26:48.059044Z","shell.execute_reply.started":"2025-09-18T01:26:47.603575Z","shell.execute_reply":"2025-09-18T01:26:48.058257Z"}},"outputs":[{"name":"stdout","text":"torch.Size([2, 384]) torch.Size([2, 384])\n","output_type":"stream"}],"execution_count":70},{"cell_type":"code","source":"sequence_ids = inputs.sequence_ids()\n# Mask everything apart from the tokens of the context\nmask = [i != 1 for i in sequence_ids]\n# Unmask the [CLS] token\nmask[0] = False\n# Mask all the [PAD] tokens\nmask = torch.logical_or(torch.tensor(mask)[None], (inputs[\"attention_mask\"] == 0))\n\nstart_logits[mask] = -10000\nend_logits[mask] = -10000","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T01:27:25.413750Z","iopub.execute_input":"2025-09-18T01:27:25.414023Z","iopub.status.idle":"2025-09-18T01:27:25.419178Z","shell.execute_reply.started":"2025-09-18T01:27:25.414004Z","shell.execute_reply":"2025-09-18T01:27:25.418577Z"}},"outputs":[],"execution_count":71},{"cell_type":"code","source":"start_probabilities = torch.nn.functional.softmax(start_logits, dim=-1)\nend_probabilities = torch.nn.functional.softmax(end_logits, dim=-1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T01:27:50.114901Z","iopub.execute_input":"2025-09-18T01:27:50.115226Z","iopub.status.idle":"2025-09-18T01:27:50.119751Z","shell.execute_reply.started":"2025-09-18T01:27:50.115205Z","shell.execute_reply":"2025-09-18T01:27:50.118939Z"}},"outputs":[],"execution_count":72},{"cell_type":"code","source":"candidates = []\nfor start_probs, end_probs in zip(start_probabilities, end_probabilities):\n    scores = start_probs[:, None] * end_probs[None, :]\n    idx = torch.triu(scores).argmax().item()\n\n    start_idx = idx // scores.shape[1]\n    end_idx = idx % scores.shape[1]\n    score = scores[start_idx, end_idx].item()\n    candidates.append((start_idx, end_idx, score))\nprint(candidates)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T01:28:10.883342Z","iopub.execute_input":"2025-09-18T01:28:10.883696Z","iopub.status.idle":"2025-09-18T01:28:10.892834Z","shell.execute_reply.started":"2025-09-18T01:28:10.883674Z","shell.execute_reply":"2025-09-18T01:28:10.892013Z"}},"outputs":[{"name":"stdout","text":"[(0, 18, 0.3386705815792084), (173, 184, 0.9714869856834412)]\n","output_type":"stream"}],"execution_count":73},{"cell_type":"code","source":"for candidate, offset in zip(candidates, offsets):\n    start_token, end_token, score = candidate\n    start_char, _ = offset[start_token]\n    _, end_char = offset[end_token]\n    answer = long_context[start_char:end_char]\n    result = {\"answer\": answer, \"start\": start_char, \"end\": end_char, \"score\": score}\n    print(result)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T01:28:49.650891Z","iopub.execute_input":"2025-09-18T01:28:49.651181Z","iopub.status.idle":"2025-09-18T01:28:49.655945Z","shell.execute_reply.started":"2025-09-18T01:28:49.651161Z","shell.execute_reply":"2025-09-18T01:28:49.655193Z"}},"outputs":[{"name":"stdout","text":"{'answer': '\\nðŸ¤— Transformers: State of the Art NLP', 'start': 0, 'end': 37, 'score': 0.3386705815792084}\n{'answer': 'Jax, PyTorch and TensorFlow', 'start': 1892, 'end': 1919, 'score': 0.9714869856834412}\n","output_type":"stream"}],"execution_count":74},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}